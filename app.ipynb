{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c55b19b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40, 3021, 23317]\n",
      "Number of tokens: 3\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "\n",
    "text = \"I love pizza\"\n",
    "tokens = encoding.encode(text)\n",
    "\n",
    "print(tokens)  # Print the list of token IDs\n",
    "print(f\"Number of tokens: {len(tokens)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da0203d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40, 3021, 23317]\n",
      "Number of tokens: 3\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-4\")\n",
    "\n",
    "text = \"I love pizza\"\n",
    "tokens = encoding.encode(text)\n",
    "\n",
    "print(tokens)  # Print the list of token IDs\n",
    "print(f\"Number of tokens: {len(tokens)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "238eb08d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40, 3047, 27941]\n",
      "Number of tokens: 3\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-4o\")\n",
    "\n",
    "text = \"I love pizza\"\n",
    "tokens = encoding.encode(text)\n",
    "\n",
    "print(tokens)  # Print the list of token IDs\n",
    "print(f\"Number of tokens: {len(tokens)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8e626ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40, 1842, 14256]\n",
      "Number of tokens: 3\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "encoding = tiktoken.encoding_for_model(\"text-davinci-003\")\n",
    "\n",
    "text = \"I love pizza\"\n",
    "tokens = encoding.encode(text)\n",
    "\n",
    "print(tokens)  # Print the list of token IDs\n",
    "print(f\"Number of tokens: {len(tokens)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "757534b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40, 3021, 23317]\n",
      "Number of tokens: 3\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "encoding = tiktoken.encoding_for_model(\"text-embedding-ada-002\")\n",
    "\n",
    "text = \"I love pizza\"\n",
    "tokens = encoding.encode(text)\n",
    "\n",
    "print(tokens)  # Print the list of token IDs\n",
    "print(f\"Number of tokens: {len(tokens)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e89754b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40, 1842, 14256]\n",
      "Number of tokens: 3\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "encoding = tiktoken.encoding_for_model(\"code-davinci-002\")\n",
    "\n",
    "text = \"I love pizza\"\n",
    "tokens = encoding.encode(text)\n",
    "\n",
    "print(tokens)  # Print the list of token IDs\n",
    "print(f\"Number of tokens: {len(tokens)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e14aebbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-3.5-turbo: [40, 3021, 23317] -> 3 tokens\n",
      "gpt-4: [40, 3021, 23317] -> 3 tokens\n",
      "gpt-4o: [40, 3047, 27941] -> 3 tokens\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "models = [\"gpt-3.5-turbo\", \"gpt-4\", \"gpt-4o\"]\n",
    "text = \"I love pizza\"\n",
    "\n",
    "for model in models:\n",
    "    enc = tiktoken.encoding_for_model(model)\n",
    "    tokens = enc.encode(text)\n",
    "    print(f\"{model}: {tokens} -> {len(tokens)} tokens\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c413dc9",
   "metadata": {},
   "source": [
    "üß† Supported Models & Their Tokenizers\n",
    "Here are some examples you can try:\n",
    "\n",
    "Model Name\tDescription\n",
    "\"gpt-3.5-turbo\"\tChatGPT 3.5\n",
    "\"gpt-4\"\tGPT-4\n",
    "\"gpt-4o\"\tGPT-4 Omni (faster, cheaper)\n",
    "\"text-davinci-003\"\tGPT-3 Completion model\n",
    "\"text-embedding-ada-002\"\tEmbedding model (different tokens)\n",
    "\"code-davinci-002\"\tCodex model for code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27e998c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ada                                 ->  3 tokens  [40, 1842, 14256]\n",
      "babbage                             ->  3 tokens  [40, 1842, 14256]\n",
      "babbage-002                         ->  3 tokens  [40, 3021, 23317]\n",
      "code-cushman-001                    ->  3 tokens  [40, 1842, 14256]\n",
      "code-cushman-002                    ->  3 tokens  [40, 1842, 14256]\n",
      "code-davinci-001                    ->  3 tokens  [40, 1842, 14256]\n",
      "code-davinci-002                    ->  3 tokens  [40, 1842, 14256]\n",
      "code-davinci-edit-001               ->  3 tokens  [40, 1842, 14256]\n",
      "code-search-ada-code-001            ->  3 tokens  [40, 1842, 14256]\n",
      "code-search-babbage-code-001        ->  3 tokens  [40, 1842, 14256]\n",
      "curie                               ->  3 tokens  [40, 1842, 14256]\n",
      "cushman-codex                       ->  3 tokens  [40, 1842, 14256]\n",
      "davinci                             ->  3 tokens  [40, 1842, 14256]\n",
      "davinci-002                         ->  3 tokens  [40, 3021, 23317]\n",
      "davinci-codex                       ->  3 tokens  [40, 1842, 14256]\n",
      "gpt-2                               ->  3 tokens  [40, 1842, 14256]\n",
      "gpt-3.5                             ->  3 tokens  [40, 3021, 23317]\n",
      "gpt-3.5-turbo                       ->  3 tokens  [40, 3021, 23317]\n",
      "gpt-35-turbo                        ->  3 tokens  [40, 3021, 23317]\n",
      "gpt-4                               ->  3 tokens  [40, 3021, 23317]\n",
      "gpt-4o                              ->  3 tokens  [40, 3047, 27941]\n",
      "gpt2                                ->  3 tokens  [40, 1842, 14256]\n",
      "o1                                  ->  3 tokens  [40, 3047, 27941]\n",
      "o3                                  ->  3 tokens  [40, 3047, 27941]\n",
      "text-ada-001                        ->  3 tokens  [40, 1842, 14256]\n",
      "text-babbage-001                    ->  3 tokens  [40, 1842, 14256]\n",
      "text-curie-001                      ->  3 tokens  [40, 1842, 14256]\n",
      "text-davinci-001                    ->  3 tokens  [40, 1842, 14256]\n",
      "text-davinci-002                    ->  3 tokens  [40, 1842, 14256]\n",
      "text-davinci-003                    ->  3 tokens  [40, 1842, 14256]\n",
      "text-davinci-edit-001               ->  3 tokens  [40, 1842, 14256]\n",
      "text-embedding-3-large              ->  3 tokens  [40, 3021, 23317]\n",
      "text-embedding-3-small              ->  3 tokens  [40, 3021, 23317]\n",
      "text-embedding-ada-002              ->  3 tokens  [40, 3021, 23317]\n",
      "text-search-ada-doc-001             ->  3 tokens  [40, 1842, 14256]\n",
      "text-search-babbage-doc-001         ->  3 tokens  [40, 1842, 14256]\n",
      "text-search-curie-doc-001           ->  3 tokens  [40, 1842, 14256]\n",
      "text-search-davinci-doc-001         ->  3 tokens  [40, 1842, 14256]\n",
      "text-similarity-ada-001             ->  3 tokens  [40, 1842, 14256]\n",
      "text-similarity-babbage-001         ->  3 tokens  [40, 1842, 14256]\n",
      "text-similarity-curie-001           ->  3 tokens  [40, 1842, 14256]\n",
      "text-similarity-davinci-001         ->  3 tokens  [40, 1842, 14256]\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "from tiktoken.model import MODEL_TO_ENCODING  # list of all models tiktoken knows\n",
    "\n",
    "text = \"I love pizza\"\n",
    "\n",
    "def encode_with_model(model: str, txt: str):\n",
    "    \"\"\"Return token ids for `txt` using the tokenizer tied to `model`.\"\"\"\n",
    "    try:\n",
    "        # Works for most models\n",
    "        enc = tiktoken.encoding_for_model(model)\n",
    "    except KeyError:\n",
    "        # Fallback: build the encoding from the raw encoding name\n",
    "        enc = tiktoken.get_encoding(MODEL_TO_ENCODING[model])\n",
    "    return enc.encode(txt)\n",
    "\n",
    "for model in sorted(MODEL_TO_ENCODING.keys()):\n",
    "    try:\n",
    "        tokens = encode_with_model(model, text)\n",
    "        print(f\"{model:35} -> {len(tokens):2} tokens  {tokens}\")\n",
    "    except Exception as e:\n",
    "        print(f\"{model:35} -> ERROR: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5bc2a6fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Hugging Face logged in\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from huggingface_hub import login\n",
    "\n",
    "load_dotenv()  # Load from .env file\n",
    "\n",
    "token = os.environ.get(\"HF_TOKEN\")\n",
    "if token:\n",
    "    login(token=token)\n",
    "    print(\"‚úÖ Hugging Face logged in\")\n",
    "else:\n",
    "    print(\"‚ùå HF_TOKEN not found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "41d7c495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-3.5-turbo                       | tiktoken     |  3 tokens | [40, 3021, 23317]\n",
      "gpt-4                               | tiktoken     |  3 tokens | [40, 3021, 23317]\n",
      "gpt-4o                              | tiktoken     |  3 tokens | [40, 3047, 27941]\n",
      "meta-llama/Meta-Llama-3-8B          | ERROR: We couldn't connect to 'https://huggingface.co' to load the files, and couldn't find them in the cached files.\n",
      "Check your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.\n",
      "mistralai/Mistral-7B-Instruct       | ERROR: mistralai/Mistral-7B-Instruct is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
      "If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "894472cf756c46b395bd1cbe4bcfefac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efb50304eaa9486c99e096f6a26e8654",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d615d91a230a492fbb1f3bc8ba4060e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/281 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiiuae/falcon-7b-instruct           | transformers |  3 tokens | [52, 1163, 12359]\n",
      "google/gemma-7b-it                  | ERROR: You are trying to access a gated repo.\n",
      "Make sure to have access to it at https://huggingface.co/google/gemma-7b-it.\n",
      "403 Client Error. (Request ID: Root=1-6887f5fb-2e962880473c681461c631fb;cd46531d-c5f6-4cec-bd41-b9aeed583dfc)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/google/gemma-7b-it/resolve/main/config.json.\n",
      "Access to model google/gemma-7b-it is restricted and you are not in the authorized list. Visit https://huggingface.co/google/gemma-7b-it to ask for access.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "from typing import Dict, Any\n",
    "from functools import lru_cache\n",
    "\n",
    "# OpenAI\n",
    "import tiktoken\n",
    "from tiktoken.model import MODEL_TO_ENCODING\n",
    "\n",
    "# Hugging Face\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "class TokenizationError(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "def is_openai_model(model_name: str) -> bool:\n",
    "    \"\"\"Decide if we should use tiktoken (OpenAI) or HF (others).\"\"\"\n",
    "    # Most robust check: does tiktoken know this model?\n",
    "    return model_name in MODEL_TO_ENCODING or model_name.startswith((\"gpt-\", \"o1\", \"o3\", \"text-davinci\"))\n",
    "\n",
    "\n",
    "@lru_cache(maxsize=None)\n",
    "def get_openai_encoder(model_name: str):\n",
    "    \"\"\"Return a tiktoken encoder for an OpenAI model name (or fall back to base encodings).\"\"\"\n",
    "    try:\n",
    "        return tiktoken.encoding_for_model(model_name)\n",
    "    except KeyError:\n",
    "        # If the exact model isn't registered, use cl100k_base (most GPT-4/3.5 models)\n",
    "        return tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "\n",
    "@lru_cache(maxsize=None)\n",
    "def get_hf_tokenizer(model_name: str):\n",
    "    \"\"\"Return a HF tokenizer (downloads if needed).\"\"\"\n",
    "    return AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "\n",
    "\n",
    "def auto_tokenize(model_name: str, text: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Tokenize `text` with the right library based on `model_name`.\n",
    "\n",
    "    Returns:\n",
    "        {\n",
    "          \"model\": ...,\n",
    "          \"library\": \"tiktoken\" | \"transformers\",\n",
    "          \"tokens\": [int, ...],\n",
    "          \"n_tokens\": int\n",
    "        }\n",
    "    \"\"\"\n",
    "    if is_openai_model(model_name):\n",
    "        enc = get_openai_encoder(model_name)\n",
    "        tokens = enc.encode(text)\n",
    "        return {\n",
    "            \"model\": model_name,\n",
    "            \"library\": \"tiktoken\",\n",
    "            \"tokens\": tokens,\n",
    "            \"n_tokens\": len(tokens),\n",
    "        }\n",
    "    else:\n",
    "        tok = get_hf_tokenizer(model_name)\n",
    "        tokens = tok.encode(text)\n",
    "        return {\n",
    "            \"model\": model_name,\n",
    "            \"library\": \"transformers\",\n",
    "            \"tokens\": tokens,\n",
    "            \"n_tokens\": len(tokens),\n",
    "        }\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    text = \"I love pizza\"\n",
    "\n",
    "    models = [\n",
    "        # OpenAI\n",
    "        \"gpt-3.5-turbo\",\n",
    "        \"gpt-4\",\n",
    "        \"gpt-4o\",\n",
    "        # Meta / Mistral / others (HF)\n",
    "        \"meta-llama/Meta-Llama-3-8B\",\n",
    "        \"mistralai/Mistral-7B-Instruct\",\n",
    "        \"tiiuae/falcon-7b-instruct\",\n",
    "        \"google/gemma-7b-it\",\n",
    "    ]\n",
    "\n",
    "    for m in models:\n",
    "        try:\n",
    "            out = auto_tokenize(m, text)\n",
    "            print(f\"{m:35} | {out['library']:12} | {out['n_tokens']:2} tokens | {out['tokens']}\")\n",
    "        except Exception as e:\n",
    "            print(f\"{m:35} | ERROR: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c3fe1216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(np.arange(10))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
